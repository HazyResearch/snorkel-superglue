{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTE Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "if not \"cwd\" in globals():\n",
    "    cwd = Path(os.getcwd())\n",
    "sys.path.insert(0, str(cwd.parents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recognizing Textual Entailment (RTE)** is a natural language inference task that addresses whether a _premise_ sentence entails a _hypothesis_ sentence, with labels `entailment` (do the facts of the premise imply the facts of the hypothesis?) or `not_entailment`.\n",
    "\n",
    "In this notebook, we'll...\n",
    "* Write over a dozen, simple slicing functions based on an [external error analysis](https://arxiv.org/abs/1904.11544) to initialize our model\n",
    "* Load existing base architecture weights + fine-tune slice heads from pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = \"RTE\"\n",
    "BERT_MODEL = \"bert-large-cased\"\n",
    "\n",
    "dataloader_config = {\n",
    "    \"batch_size\": 8,\n",
    "    \"data_dir\": os.environ.get(\"SUPERGLUEDATA\", os.path.join(str(cwd.parents[0]), \"data\")),\n",
    "    \"splits\": [\"train\", \"valid\"],\n",
    "}\n",
    "\n",
    "trainer_config = {\n",
    "    \"lr\": 2e-5,\n",
    "    \"optimizer\": \"adamax\",\n",
    "    \"n_epochs\": 15,\n",
    "    \"conter_unit\": \"epochs\",\n",
    "    \"evaluation_freq\": 0.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_dataloaders\n",
    "\n",
    "dataloaders = get_dataloaders(\n",
    "    task_name=TASK_NAME,\n",
    "    tokenizer_name=BERT_MODEL,\n",
    "    **dataloader_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1242874899/1242874899 [01:18<00:00, 15835137.96B/s]\n"
     ]
    }
   ],
   "source": [
    "from superglue_tasks import task_funcs\n",
    "\n",
    "task = task_funcs[TASK_NAME](BERT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.mtl.model import MultitaskModel\n",
    "from snorkel.mtl.trainer import Trainer\n",
    "\n",
    "vanilla_model = MultitaskModel(tasks=[task], device=-1, dataparallel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following to train the vanilla BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(**trainer_config)\n",
    "# trainer.train_model(model, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, use the one we pretrained for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're missing the model, uncomment this line:\n",
    "# ! wget https://www.dropbox.com/s/t2ri9o0iz765hsn/RTE_bert.pth?dl=0 && mv RTE_bert.pth?dl=0 RTE_bert.pth\n",
    "\n",
    "vanilla_model.load(\"RTE_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 49s, sys: 23.8 s, total: 4min 13s\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RTE/SuperGLUE/valid/accuracy': 0.7364620938628159}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vanilla_model.score(dataloaders[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SFs\n",
    "We emphasize here that our _data slicing_ abstraction follows intuitive, programmer workflows!\n",
    "\n",
    "We rely on error buckets defined by [Kim et. al 2019](https://arxiv.org/pdf/1904.11544.pdf) to define our slices.\n",
    "Then, we apply quick-to-write, heuristics to target each of these buckets. \n",
    "Intuitively, these are slices that were important enough to be measured independently by researchers—so we'd like to write slicing functions to try and improve performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing.sf import slicing_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def slice_temporal_preposition(example):\n",
    "    temporal_prepositions = [\"after\", \"before\", \"past\"]\n",
    "    both_sentences = example.sentence1 + example.sentence2\n",
    "    return any([p in both_sentences for p in temporal_prepositions])\n",
    "\n",
    "@slicing_function()\n",
    "def slice_possessive_preposition(example):\n",
    "    possessive_prepositions = [\"inside of\", \"with\", \"within\"]\n",
    "    both_sentences = example.sentence1 + example.sentence2\n",
    "    return any([p in both_sentences for p in possessive_prepositions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def slice_is_comparative(example):\n",
    "    comparative_words = [\"more\", \"less\", \"better\", \"worse\", \"bigger\", \"smaller\"]\n",
    "    both_sentences = example.sentence1 + example.sentence2\n",
    "    return any([p in both_sentences for p in comparative_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def slice_is_quantification(example):\n",
    "    quantification_words = [\"all\", \"some\", \"none\"]\n",
    "    both_sentences = example.sentence1 + example.sentence2\n",
    "    return any([p in both_sentences for p in quantification_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wh-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def slice_where(example):\n",
    "    sentences = example.sentence1 + example.sentence2\n",
    "    return \"where\" in sentences\n",
    "\n",
    "@slicing_function()\n",
    "def slice_who(example):\n",
    "    sentences = example.sentence1 + example.sentence2\n",
    "    return \"who\" in sentences\n",
    "\n",
    "@slicing_function()\n",
    "def slice_what(example):\n",
    "    sentences = example.sentence1 + example.sentence2\n",
    "    return \"what\" in sentences\n",
    "\n",
    "@slicing_function()\n",
    "def slice_when(example):\n",
    "    sentences = example.sentence1 + example.sentence2\n",
    "    return \"when\" in sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinating Conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def slice_and(example):\n",
    "    sentences = example.sentence1 + example.sentence2\n",
    "    return \"and\" in sentences\n",
    "\n",
    "@slicing_function()\n",
    "def slice_but(example):\n",
    "    sentences = example.sentence1 + example.sentence2\n",
    "    return \"but\" in sentences\n",
    "\n",
    "@slicing_function()\n",
    "def slice_or(example):\n",
    "    sentences = example.sentence1 + example.sentence2\n",
    "    return \"or\" in sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definite-Indefinite Articles\n",
    "Multiple occurences of articles like `the` or `an` might refer to different entities—we try to heuristically capture this here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def slice_multiple_articles(example):\n",
    "    sentences = example.sentence1 + example.sentence2\n",
    "    multiple_indefinite = sum([int(x == \"a\") for x in sentences.split()]) > 1 \\\n",
    "        or sum([int(x == \"an\") for x in sentences.split()]) > 1\n",
    "    multiple_definite = sum([int(x == \"the\") for x in sentences.split()]) > 1\n",
    "    return multiple_indefinite or multiple_definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Misc.) Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def slice_short_hypothesis(example, thresh=5):\n",
    "    return len(example.sentence2.split()) < thresh\n",
    "\n",
    "@slicing_function()\n",
    "def slice_long_hypothesis(example, thresh=15):\n",
    "    return len(example.sentence2.split()) > thresh\n",
    "\n",
    "@slicing_function()\n",
    "def slice_short_premise(example, thresh=10):\n",
    "    return len(example.sentence1.split()) < thresh\n",
    "\n",
    "@slicing_function()\n",
    "def slice_long_premise(example, thresh=100):\n",
    "    return len(example.sentence1.split()) > thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add slices to dataloaders and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_functions = [\n",
    "    slice_temporal_preposition,\n",
    "    slice_possessive_preposition,\n",
    "    slice_is_comparative,\n",
    "    slice_is_quantification,\n",
    "    slice_where,\n",
    "    slice_who,\n",
    "    slice_what,\n",
    "    slice_when,\n",
    "    slice_and,\n",
    "    slice_or,\n",
    "    slice_but,\n",
    "    slice_multiple_articles,\n",
    "    slice_short_hypothesis,\n",
    "    slice_long_hypothesis,\n",
    "    slice_short_premise,\n",
    "    slice_long_premise\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_names = [sf.name for sf in slicing_functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(name=RTE_slice:slice_temporal_preposition_ind),\n",
       " Task(name=RTE_slice:slice_possessive_preposition_ind),\n",
       " Task(name=RTE_slice:slice_is_comparative_ind),\n",
       " Task(name=RTE_slice:slice_is_quantification_ind),\n",
       " Task(name=RTE_slice:slice_where_ind),\n",
       " Task(name=RTE_slice:slice_who_ind),\n",
       " Task(name=RTE_slice:slice_what_ind),\n",
       " Task(name=RTE_slice:slice_when_ind),\n",
       " Task(name=RTE_slice:slice_and_ind),\n",
       " Task(name=RTE_slice:slice_or_ind),\n",
       " Task(name=RTE_slice:slice_but_ind),\n",
       " Task(name=RTE_slice:slice_multiple_articles_ind),\n",
       " Task(name=RTE_slice:slice_short_hypothesis_ind),\n",
       " Task(name=RTE_slice:slice_long_hypothesis_ind),\n",
       " Task(name=RTE_slice:slice_short_premise_ind),\n",
       " Task(name=RTE_slice:slice_long_premise_ind),\n",
       " Task(name=RTE_slice:base_ind),\n",
       " Task(name=RTE_slice:slice_temporal_preposition_pred),\n",
       " Task(name=RTE_slice:slice_possessive_preposition_pred),\n",
       " Task(name=RTE_slice:slice_is_comparative_pred),\n",
       " Task(name=RTE_slice:slice_is_quantification_pred),\n",
       " Task(name=RTE_slice:slice_where_pred),\n",
       " Task(name=RTE_slice:slice_who_pred),\n",
       " Task(name=RTE_slice:slice_what_pred),\n",
       " Task(name=RTE_slice:slice_when_pred),\n",
       " Task(name=RTE_slice:slice_and_pred),\n",
       " Task(name=RTE_slice:slice_or_pred),\n",
       " Task(name=RTE_slice:slice_but_pred),\n",
       " Task(name=RTE_slice:slice_multiple_articles_pred),\n",
       " Task(name=RTE_slice:slice_short_hypothesis_pred),\n",
       " Task(name=RTE_slice:slice_long_hypothesis_pred),\n",
       " Task(name=RTE_slice:slice_short_premise_pred),\n",
       " Task(name=RTE_slice:slice_long_premise_pred),\n",
       " Task(name=RTE_slice:base_pred),\n",
       " Task(name=RTE)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.slicing.utils import add_slice_labels, convert_to_slice_tasks\n",
    "\n",
    "# make slices tasks\n",
    "slice_tasks = convert_to_slice_tasks(task, slice_names)\n",
    "slice_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2490/2490 [00:00<00:00, 3484.15it/s]\n",
      "100%|██████████| 277/277 [00:00<00:00, 3419.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.slicing.apply import PandasSFApplier\n",
    "from snorkel.slicing.utils import add_slice_labels\n",
    "from utils import task_dataset_to_dataframe\n",
    "\n",
    "applier = PandasSFApplier(slicing_functions)\n",
    "\n",
    "# add slice labels\n",
    "for dl in dataloaders:\n",
    "    df = task_dataset_to_dataframe(dl.dataset)\n",
    "    S_matrix = applier.apply(df)\n",
    "    \n",
    "    # updates dataloaders in place\n",
    "    add_slice_labels(dl, task, S_matrix, slice_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_model = MultitaskModel(tasks=slice_tasks, device=-1, dataparallel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from pretrained BERT\n",
    "Given that the `slice_model` shares the same backbone architecture as the `vanilla_model`, we can simply reload these backbone weights (pretrained on RTE), and then fine-tune the slicing heads!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previous backbone weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_model.load_state_dict(vanilla_model.collect_state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fine-tune the slice heads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(**trainer_config)\n",
    "# trainer.train_model(slice_model, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load our pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're missing the model, uncomment this line:\n",
    "# ! wget https://www.dropbox.com/s/18ta5z3tzasba0m/RTE_slice_from_bert.pth?dl=0 && mv RTE_slice_from_bert.pth?dl=0 RTE_slice_from_bert.pth\n",
    "\n",
    "slice_model.load(\"RTE_slice_from_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/snorkel/lib/python3.6/site-packages/snorkel/slicing/modules/slice_combiner.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for slice_ind_name in slice_ind_op_names\n",
      "/home/ubuntu/anaconda3/envs/snorkel/lib/python3.6/site-packages/snorkel/slicing/modules/slice_combiner.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for slice_pred_name in slice_pred_op_names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 1s, sys: 21 s, total: 4min 22s\n",
      "Wall time: 1min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RTE/SuperGLUE/valid/accuracy': 0.7581227436823105,\n",
       " 'RTE_slice:slice_temporal_preposition_ind/SuperGLUE/valid/f1': 0.1951219512195122,\n",
       " 'RTE_slice:slice_temporal_preposition_pred/SuperGLUE/valid/accuracy': 0.8333333333333334,\n",
       " 'RTE_slice:slice_possessive_preposition_ind/SuperGLUE/valid/f1': 0.29885057471264365,\n",
       " 'RTE_slice:slice_possessive_preposition_pred/SuperGLUE/valid/accuracy': 0.696969696969697,\n",
       " 'RTE_slice:slice_is_comparative_ind/SuperGLUE/valid/f1': 0,\n",
       " 'RTE_slice:slice_is_comparative_pred/SuperGLUE/valid/accuracy': 0.7096774193548387,\n",
       " 'RTE_slice:slice_is_quantification_ind/SuperGLUE/valid/f1': 0.29059829059829057,\n",
       " 'RTE_slice:slice_is_quantification_pred/SuperGLUE/valid/accuracy': 0.6818181818181818,\n",
       " 'RTE_slice:slice_where_ind/SuperGLUE/valid/f1': 0,\n",
       " 'RTE_slice:slice_where_pred/SuperGLUE/valid/accuracy': 0.7777777777777778,\n",
       " 'RTE_slice:slice_who_ind/SuperGLUE/valid/f1': 0.24,\n",
       " 'RTE_slice:slice_who_pred/SuperGLUE/valid/accuracy': 0.7222222222222222,\n",
       " 'RTE_slice:slice_what_ind/SuperGLUE/valid/f1': 0,\n",
       " 'RTE_slice:slice_what_pred/SuperGLUE/valid/accuracy': 0.2857142857142857,\n",
       " 'RTE_slice:slice_when_ind/SuperGLUE/valid/f1': 0,\n",
       " 'RTE_slice:slice_when_pred/SuperGLUE/valid/accuracy': 0.75,\n",
       " 'RTE_slice:slice_and_ind/SuperGLUE/valid/f1': 0.8088642659279779,\n",
       " 'RTE_slice:slice_and_pred/SuperGLUE/valid/accuracy': 0.7692307692307693,\n",
       " 'RTE_slice:slice_or_ind/SuperGLUE/valid/f1': 0.8952772073921972,\n",
       " 'RTE_slice:slice_or_pred/SuperGLUE/valid/accuracy': 0.7443946188340808,\n",
       " 'RTE_slice:slice_but_ind/SuperGLUE/valid/f1': 0,\n",
       " 'RTE_slice:slice_but_pred/SuperGLUE/valid/accuracy': 0.6538461538461539,\n",
       " 'RTE_slice:slice_multiple_articles_ind/SuperGLUE/valid/f1': 0.8658823529411765,\n",
       " 'RTE_slice:slice_multiple_articles_pred/SuperGLUE/valid/accuracy': 0.7653061224489796,\n",
       " 'RTE_slice:slice_short_hypothesis_ind/SuperGLUE/valid/f1': 0.33333333333333337,\n",
       " 'RTE_slice:slice_short_hypothesis_pred/SuperGLUE/valid/accuracy': 0.6666666666666666,\n",
       " 'RTE_slice:slice_long_hypothesis_ind/SuperGLUE/valid/f1': 0.39999999999999997,\n",
       " 'RTE_slice:slice_long_hypothesis_pred/SuperGLUE/valid/accuracy': 0.8181818181818182,\n",
       " 'RTE_slice:slice_short_premise_ind/SuperGLUE/valid/f1': 0,\n",
       " 'RTE_slice:slice_short_premise_pred/SuperGLUE/valid/accuracy': 0.8333333333333334,\n",
       " 'RTE_slice:slice_long_premise_ind/SuperGLUE/valid/f1': 0.8510638297872342,\n",
       " 'RTE_slice:slice_long_premise_pred/SuperGLUE/valid/accuracy': 0.6818181818181818,\n",
       " 'RTE_slice:base_ind/SuperGLUE/valid/f1': 1.0,\n",
       " 'RTE_slice:base_pred/SuperGLUE/valid/accuracy': 0.7581227436823105}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "slice_model.score(dataloaders[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying all these slicing functions at the model (in a type of shotgun approach...) we see overall improvements of **+2.2 accuracy points**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snorkel]",
   "language": "python",
   "name": "conda-env-snorkel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
