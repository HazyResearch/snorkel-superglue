{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel Workshop: Augmentation Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Getting started\n",
    "\n",
    "In this tutorial, we'll explore augmenting training datasets using transformation functions (TFs). We'll focus on the Words in Context task from SuperGLUE. But first, we'll take care of a few imports and defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from snorkel.augmentation.apply import PandasTFApplier\n",
    "from snorkel.augmentation.policy import RandomAugmentationPolicy\n",
    "from snorkel.augmentation.tf import transformation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"cwd\" in globals():\n",
    "    cwd = Path(os.getcwd())\n",
    "sys.path.insert(0, str(cwd.parents[0]))\n",
    "\n",
    "from dataloaders import get_jsonl_path\n",
    "from superglue_parsers.wic import get_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "task_name = \"WiC\"\n",
    "data_dir = os.environ[\"SUPERGLUEDATA\"]\n",
    "split = \"train\"\n",
    "max_data_samples = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We'll load the WiC data from our local download and construct a Pandas DataFrame with it. Just as a quick check, let's take a look at some of the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jsonl_path = get_jsonl_path(data_dir, task_name, split)\n",
    "wic_df = pd.DataFrame.from_records(get_rows(jsonl_path, max_data_samples))\n",
    "wic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing transformation functions\n",
    "\n",
    "Let's write our first transformation function. A common approach in NLP tasks is to replace important words with synonyms. Here, we'll replace the keyword in the two sentences with a new word randomly sampled from a synonym set. We'll filter out complicated phrases and different parts-of-speech from our synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll write a helper function to execute the core logic of our TF. Given the key word and its part-of-speech, it calls `nltk`'s wordnet tooling to create a filtered set of synonym words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "def get_filtered_syns(word, pos):\n",
    "    # Use Wordnet to find synonyms and filter out\n",
    "    # synonyms that are\n",
    "    #  * the same word as the original\n",
    "    #  * composed of multiple words\n",
    "    #  * different POS from the original\n",
    "    syns = wordnet.synsets(word)\n",
    "    syns_filtered = set()\n",
    "    for s in syns:\n",
    "        name_parts = s.name().split(\".\")\n",
    "        s_word = name_parts[0]\n",
    "        same_pos = name_parts[1] == pos.lower()\n",
    "        if s_word != word and (\"_\" not in s_word) and same_pos:\n",
    "            syns_filtered.add(s_word)\n",
    "    return list(syns_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a simple example to verify functionality. As expected, we get different synonyms for when \"stream\" is used as a verb and as a noun. Try out a few more words. It's important to note that this method doesn't provide perfect substitutions. However, they can still help with training. For more information, see [this blog post](https://towardsdatascience.com/these-are-the-easiest-data-augmentation-techniques-in-natural-language-processing-you-can-think-of-88e393fd610)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = \"stream\"\n",
    "\n",
    "print(f\"Synonyms for '{word}' (verb):\", get_filtered_syns(word, \"V\"))\n",
    "print(f\"Synonyms for '{word}' (noun):\", get_filtered_syns(word, \"N\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll wrap out helper in a transformation function. In addition to sampling from the generated synonym set, we need to reconstruct our example. Note that the TF returns `None` if there's no available transformation. This happens if there are no valid synonyms, or if the key word appears in different forms between the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transformation_function\n",
    "def replace_word(x):\n",
    "    # Break up sentence into tokens\n",
    "    sentence1_tokens = x.sentence1.split()\n",
    "    sentence2_tokens = x.sentence2.split()\n",
    "    sentence1_instance = sentence1_tokens[x.sentence1_idx]\n",
    "    sentence2_instance = sentence2_tokens[x.sentence2_idx]\n",
    "    # Check if any word forms are different\n",
    "    if len({sentence1_instance, sentence2_instance, x.word}) > 1:\n",
    "        return None\n",
    "    # Get and filter synonyms, then randomly sample\n",
    "    syns = get_filtered_syns(x.word, x.pos)\n",
    "    if len(syns) == 0:\n",
    "        return None\n",
    "    syn = random.choice(syns)\n",
    "    # Swap in synonym\n",
    "    sentence1_tokens[x.sentence1_idx] = syn\n",
    "    sentence2_tokens[x.sentence2_idx] = syn\n",
    "    # Reconstruct example and return\n",
    "    x.sentence1 = \" \".join(sentence1_tokens)\n",
    "    x.sentence2 = \" \".join(sentence2_tokens)\n",
    "    x.word = syn\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our transformation function\n",
    "\n",
    "In order to apply our TF, we need two things: a policy and an applier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Policy_\n",
    "\n",
    "The policy dictates how the TFs should be composed in a sequence. Since we only have one TF, we can use just about any policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = [replace_word]\n",
    "policy = RandomAugmentationPolicy(len([replace_word]), sequence_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Applier_\n",
    "\n",
    "The applier takes our TFs and policy, and applies them to a DataFrame of examples. We'll specify that we want 1 transformed example per original, and that we want to keep the original as well. If our TF returns `None`, there won't be a transformed example in our output DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "applier = PandasTFApplier(tfs, policy, k=1, keep_original=True)\n",
    "wic_df_synonym = applier.apply(wic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wic_df_synonym.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing more transformation functions\n",
    "\n",
    "This is perhaps the simplest TF we can write for WiC. Since the sentences are unordered, we can just swap them to generate a new example. We'll apply this to our DataFrame with synonym-swapped examples as well so that we get new examples for those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transformation_function\n",
    "def swap_sentences(x):\n",
    "    x.sentence1, x.sentence2 = x.sentence2, x.sentence1\n",
    "    x.sentence1_idx, x.sentence2_idx = x.sentence2_idx, x.sentence1_idx\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll define our policy and applier, then create our augmented DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = [swap_sentences]\n",
    "policy = RandomAugmentationPolicy(len(tfs), sequence_length=1)\n",
    "applier = PandasTFApplier(tfs, policy, k=1, keep_original=True)\n",
    "wic_df_swapped = applier.apply(wic_df_synonym)\n",
    "wic_df_swapped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's your turn!\n",
    "\n",
    "Try writing a transformation function of your own! Remember, it should output either a new example or `None`. Get creative! Just like we wrapped a resource from `nltk` in our synonym-swapping TF, we can wrap any other existing language model, etc. For more ideas, check out this [blog post](https://towardsdatascience.com/these-are-the-easiest-data-augmentation-techniques-in-natural-language-processing-you-can-think-of-88e393fd610)\n",
    "or this [more advanced blog post](https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@transformation_function\n",
    "def my_tf(x):\n",
    "    return x\n",
    "    \n",
    "tfs = [replace_word, my_tf]\n",
    "policy = RandomAugmentationPolicy(len(tfs), sequence_length=2)\n",
    "applier = PandasTFApplier(tfs, policy, k=2, keep_original=True)\n",
    "wic_df_augmented = applier.apply(wic_df)\n",
    "wic_df_augmented.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with augmented data\n",
    "\n",
    "Feeling ambitious? Try training a WiC model with your augmented data.\n",
    "\n",
    "**_Important_**: to get the full training set, you'll need to re-execute from the beginning and set `max_data_samples` to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll construct our dataset with the default helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.mtl.data import MultitaskDataLoader\n",
    "from snorkel.mtl.model import MultitaskModel\n",
    "from snorkel.mtl.snorkel_config import default_config as config\n",
    "from snorkel.mtl.trainer import Trainer\n",
    "\n",
    "import superglue_tasks\n",
    "from dataloaders import get_dataloaders\n",
    "from superglue_parsers.wic import parse_from_rows\n",
    "from tokenizer import get_tokenizer\n",
    "\n",
    "\n",
    "max_sequence_length = 256\n",
    "batch_size = 4\n",
    "tokenizer_name = \"bert-large-cased\"\n",
    "tokenizer = get_tokenizer(tokenizer_name)\n",
    "\n",
    "# Construct training dataloader from augmented DF\n",
    "rows = wic_df_swapped.to_dict(\"records\")\n",
    "dataset = parse_from_rows(rows, tokenizer, max_sequence_length)\n",
    "train_dataloader = MultitaskDataLoader(\n",
    "    task_to_label_dict={task_name: \"labels\"},\n",
    "    dataset=dataset,\n",
    "    split=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_dataloader = get_dataloaders(\n",
    "    data_dir,\n",
    "    task_name=task_name,\n",
    "    splits=[\"valid\"],\n",
    "    max_data_samples=None,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    batch_size=batch_size,\n",
    ")[0]\n",
    "\n",
    "dataloaders = [train_dataloader, valid_dataloader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the slicing tutorial, we'll use the Snorkel API to configure a BERT model to train our natural language understanding model. This again comes from [huggingface's BERT library](https://github.com/huggingface/pytorch-pretrained-BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-large-cased\"\n",
    "base_task = superglue_tasks.task_funcs[task_name](bert_model)\n",
    "tasks = [base_task]\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultitaskModel(\n",
    "    name=f\"SuperGLUE\",\n",
    "    tasks=tasks, \n",
    "    dataparallel=False,\n",
    "    device=-1 # use CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to uncomment this block to experiment with it yourself! It will take a while to train on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(**config)\n",
    "# trainer.train_model(model, dataloaders)\n",
    "# model.save(\"./model_with_data_augmentation.pth\")\n",
    "# model.score(dataloaders[1])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
