{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel Workshop: Slicing Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To start, let's make sure that we have the right paths/environment variables set by following the instructions in `snorkel-superglue/README.md`.\n",
    "\n",
    "Specifically, ensure that (1) `snorkel` is installed and (2) `SUPERGLUEDATA` is set where [download_superglue_data.py](https://github.com/HazyResearch/snorkel-superglue/blob/staging/download_superglue_data.py) was called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "if not \"cwd\" in globals():\n",
    "    cwd = Path(os.getcwd())\n",
    "sys.path.insert(0, str(cwd.parents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Don't truncate the sentence when viewing examples\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we rely heavily on the `snorkel.mtl` module, which is a great abstraction for implementing these slicing tasks. \n",
    "Intuitively, we want an API to additional capacity corresponding to each slice—exactly what the task flows in these packages allow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.mtl.data import MultitaskDataLoader\n",
    "from snorkel.mtl.model import MultitaskModel\n",
    "from snorkel.mtl.snorkel_config import default_config as config\n",
    "from snorkel.mtl.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "from tokenizer import get_tokenizer\n",
    "from utils import task_dataset_to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the WiC dataset\n",
    "We'll be working with the [Words in Context (WiC) task](https://pilehvar.github.io/wic/). To start, let's look at a few examples. To do so, we'll convert them to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_jsonl_path\n",
    "from parsers.wic import get_rows\n",
    "\n",
    "task_name = \"WiC\"\n",
    "data_dir = os.environ[\"SUPERGLUEDATA\"]\n",
    "split = \"valid\"\n",
    "max_data_samples = None # max examples to include in dataset\n",
    "\n",
    "jsonl_path = get_jsonl_path(data_dir, task_name, split)\n",
    "wic_df = pd.DataFrame.from_records(get_rows(jsonl_path, max_data_samples=max_data_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, the WiC task is used to identify the intended meaning of specified words across multiple contexts—the `label` indicates whether the word is used in the same sense in both `sentence1` and `sentence2`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Room and board .</td>\n",
       "      <td>He nailed boards across the windows .</td>\n",
       "      <td>board</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circulate a rumor .</td>\n",
       "      <td>This letter is being circulated among the faculty .</td>\n",
       "      <td>circulate</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hook a fish .</td>\n",
       "      <td>He hooked a snake accidentally , and was so scared he dropped his rod into the water .</td>\n",
       "      <td>hook</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For recreation he wrote poetry and solved crossword puzzles .</td>\n",
       "      <td>Drug abuse is often regarded as a form of recreation .</td>\n",
       "      <td>recreation</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Making a hobby of domesticity .</td>\n",
       "      <td>A royal family living in unpretentious domesticity .</td>\n",
       "      <td>domesticity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       sentence1  \\\n",
       "0  Room and board .                                                \n",
       "1  Circulate a rumor .                                             \n",
       "2  Hook a fish .                                                   \n",
       "3  For recreation he wrote poetry and solved crossword puzzles .   \n",
       "4  Making a hobby of domesticity .                                 \n",
       "\n",
       "                                                                                sentence2  \\\n",
       "0  He nailed boards across the windows .                                                    \n",
       "1  This letter is being circulated among the faculty .                                      \n",
       "2  He hooked a snake accidentally , and was so scared he dropped his rod into the water .   \n",
       "3  Drug abuse is often regarded as a form of recreation .                                   \n",
       "4  A royal family living in unpretentious domesticity .                                     \n",
       "\n",
       "          word  label  \n",
       "0  board        False  \n",
       "1  circulate    False  \n",
       "2  hook         True   \n",
       "3  recreation   True   \n",
       "4  domesticity  False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wic_df[[\"sentence1\", \"sentence2\", \"word\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using BERT\n",
    "Now, let's train a model using the Snorkel API, with the [BERT](https://arxiv.org/abs/1810.04805) model, a powerful pre-training mechanism for general language understanding.\n",
    "Thanks to folks at [huggingface](https://github.com/huggingface/pytorch-pretrained-BERT), we can use this model with with a simple import statement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-large-cased\"\n",
    "tokenizer_name = \"bert-large-cased\"\n",
    "batch_size = 4\n",
    "max_sequence_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word-piece tokenizer for the 'bert-large-cased' vocabulary\n",
    "tokenizer = get_tokenizer(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the style of our MTL tutorial, we'll use a few helpers to load them into Pytorch datasets/our `MultitaskDataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_dataset\n",
    "\n",
    "datasets = []\n",
    "dataloaders = []\n",
    "for split in [\"train\", \"valid\"]:\n",
    "    # parse raw data and format it as a Pytorch dataset\n",
    "    dataset = get_dataset(\n",
    "        data_dir, task_name, split, tokenizer, max_data_samples, max_sequence_length\n",
    "    )\n",
    "    dataloader = MultitaskDataLoader(\n",
    "        task_to_label_dict={task_name: \"labels\"},\n",
    "        dataset=dataset,\n",
    "        split=split,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(split == \"train\"),\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "    dataloaders.append(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(name=WiC)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct dataloaders and tasks and load slices\n",
    "base_task = models.model[task_name](bert_model)\n",
    "tasks = [base_task]\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultitaskModel(\n",
    "    name=f\"SuperGLUE\",\n",
    "    tasks=tasks, \n",
    "    dataparallel=False,\n",
    "    device=-1 # use CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've pretrained a model for you, but feel free to uncomment this line to experiment with it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(**config)\n",
    "# trainer.train_model(slice_model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're missing the model, uncomment this line:\n",
    "# ! wget https://www.dropbox.com/s/vix9bhzy18o3wjl/vanilla_model.pth?dl=0 && mv vanilla_model.pth?dl=0 vanilla_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wic_path = \"vanilla_model.pth\"\n",
    "model.load(wic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do we do on the valid set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 50s, sys: 1.83 s, total: 13min 51s\n",
      "Wall time: 30.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'WiC/SuperGLUE/valid/accuracy': 0.7460815047021944}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.score(dataloaders[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis (specific to the SFs we plan to write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to debugging machine learning models---error analysis! let's look at a few examples that we get wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 24s, sys: 5.99 s, total: 13min 30s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = model.predict(dataloaders[1], return_preds=True)\n",
    "golds, preds = results[\"golds\"][task_name], results[\"preds\"][task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circulate a rumor .</td>\n",
       "      <td>This letter is being circulated among the faculty .</td>\n",
       "      <td>circulate</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Making a hobby of domesticity .</td>\n",
       "      <td>A royal family living in unpretentious domesticity .</td>\n",
       "      <td>domesticity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The child 's acquisition of language .</td>\n",
       "      <td>That graphite tennis racquet is quite an acquisition .</td>\n",
       "      <td>acquisition</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>They swam in the nude .</td>\n",
       "      <td>The marketing rule ' nude sells ' spread from verbal to visual mainstream media in the 20th century .</td>\n",
       "      <td>nude</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>He took the manuscript in both hands and gave it a mighty tear .</td>\n",
       "      <td>There were big tears rolling down Lisa 's cheeks .</td>\n",
       "      <td>tear</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           sentence1  \\\n",
       "1   Circulate a rumor .                                                \n",
       "4   Making a hobby of domesticity .                                    \n",
       "5   The child 's acquisition of language .                             \n",
       "7   They swam in the nude .                                            \n",
       "16  He took the manuscript in both hands and gave it a mighty tear .   \n",
       "\n",
       "                                                                                                sentence2  \\\n",
       "1   This letter is being circulated among the faculty .                                                     \n",
       "4   A royal family living in unpretentious domesticity .                                                    \n",
       "5   That graphite tennis racquet is quite an acquisition .                                                  \n",
       "7   The marketing rule ' nude sells ' spread from verbal to visual mainstream media in the 20th century .   \n",
       "16  There were big tears rolling down Lisa 's cheeks .                                                      \n",
       "\n",
       "           word  label  \n",
       "1   circulate    False  \n",
       "4   domesticity  False  \n",
       "5   acquisition  False  \n",
       "7   nude         False  \n",
       "16  tear         False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_preds = golds != preds\n",
    "wic_df[incorrect_preds][[\"sentence1\", \"sentence2\", \"word\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that one particular error mode occurs when the target **word** is a _verb_. Let's investigate further...\n",
    "\n",
    "We view examples where we make the wrong prediction _and_ the target word is a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Circulate a rumor .</td>\n",
       "      <td>This letter is being circulated among the faculty .</td>\n",
       "      <td>circulate</td>\n",
       "      <td>V</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>To clutch power .</td>\n",
       "      <td>She clutched her purse .</td>\n",
       "      <td>clutch</td>\n",
       "      <td>V</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>She used to wait down at the Dew Drop Inn .</td>\n",
       "      <td>Wait here until your car arrives .</td>\n",
       "      <td>wait</td>\n",
       "      <td>V</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Wear gloves so your hands stay warm .</td>\n",
       "      <td>Stay with me , please .</td>\n",
       "      <td>stay</td>\n",
       "      <td>V</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>You need to push quite hard to get this door open .</td>\n",
       "      <td>Nora pushed through the crowd .</td>\n",
       "      <td>push</td>\n",
       "      <td>V</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence1  \\\n",
       "1   Circulate a rumor .                                   \n",
       "45  To clutch power .                                     \n",
       "62  She used to wait down at the Dew Drop Inn .           \n",
       "78  Wear gloves so your hands stay warm .                 \n",
       "83  You need to push quite hard to get this door open .   \n",
       "\n",
       "                                              sentence2       word pos  label  \n",
       "1   This letter is being circulated among the faculty .  circulate  V   False  \n",
       "45  She clutched her purse .                             clutch     V   True   \n",
       "62  Wait here until your car arrives .                   wait       V   False  \n",
       "78  Stay with me , please .                              stay       V   True   \n",
       "83  Nora pushed through the crowd .                      push       V   True   "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_is_verb = wic_df[\"pos\"] == \"V\"\n",
    "df_wrong_and_target_is_verb = wic_df[incorrect_preds & target_is_verb]\n",
    "df_wrong_and_target_is_verb[[\"sentence1\", \"sentence2\", \"word\", \"pos\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3765432098765432"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_wrong_and_target_is_verb) / len(wic_df[incorrect_preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error mode accounts for over **37%** of our incorrect predictions! Let's address with _slicing_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write slicing functions\n",
    "We write slicing functions to target specific subsets of the data that we care about—this could correspond to the examples we find underperforming in an error analysis, or specific subsets that are application critical (e.g. night-time images in a self-driving dataset). Then, we'd like to add slice-specific capacity to our model so that it pays more attention to these examples!\n",
    "\n",
    "By applying the decorator, `@slicing_function()`, we wrap each Snorkel to have access to previously defined preprocessors, resources, etc.— just like with labeling fucntions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing.sf import slicing_function\n",
    "from snorkel.types import DataPoint\n",
    "\n",
    "@slicing_function()\n",
    "def SF_verb(x: DataPoint) -> int:\n",
    "    return x.pos == 'V'\n",
    "\n",
    "slicing_functions = [SF_verb]\n",
    "slice_names = [sf.name for sf in slicing_functions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train _slice-aware_ model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's update our tasks to to add _additional capacity_ corresponding to each slice we've specified.\n",
    "At a high level, `convert_to_slicing_tasks` will take an existing task and add additional Pytorch modules in the task flow corresponding to each of the `slice_names` you've provided.\n",
    "\n",
    "Note that this plays nicely into our MTL abstraction—additional operations in the task flow are specific for each slice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(name=WiC_slice:SF_verb_ind),\n",
       " Task(name=WiC_slice:base_ind),\n",
       " Task(name=WiC_slice:SF_verb_pred),\n",
       " Task(name=WiC_slice:base_pred),\n",
       " Task(name=WiC)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.slicing.utils import convert_to_slice_tasks\n",
    "\n",
    "slice_tasks = convert_to_slice_tasks(base_task, slice_names)\n",
    "slice_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then update our dataloaders so that our labels are set up to appropriately train on these slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5428/5428 [00:00<00:00, 24523.58it/s]\n",
      "100%|██████████| 638/638 [00:00<00:00, 41594.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.slicing.apply import PandasSFApplier\n",
    "from snorkel.slicing.utils import add_slice_labels\n",
    "\n",
    "slice_dataloaders = []\n",
    "applier = PandasSFApplier(slicing_functions)\n",
    "\n",
    "for dl in dataloaders:\n",
    "    df = task_dataset_to_dataframe(dl.dataset)\n",
    "    S_matrix = applier.apply(df)\n",
    "    # updates dataloaders in place\n",
    "    add_slice_labels(dl, base_task, S_matrix, slice_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a new _slice-aware model_, and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_model = MultitaskModel(\n",
    "    name=f\"SuperGLUE\", \n",
    "    tasks=slice_tasks, \n",
    "    dataparallel=False,\n",
    "    device=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we've loaded a pretrained model for you to explore on your own, but you can explore training if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(**config)\n",
    "# trainer.train_model(slice_model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're missing the model, uncomment this line:\n",
    "# ! wget https://www.dropbox.com/s/h6620vfeompgu9o/slice_model.pth?dl=0 && mv slice_model.pth?dl=0 slice_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_wic_path = \"slice_model.pth\"\n",
    "slice_model.load(slice_wic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate _slice-aware_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/vschen/snorkel/snorkel/slicing/modules/slice_combiner.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for slice_ind_name in slice_ind_op_names\n",
      "/dfs/scratch0/vschen/snorkel/snorkel/slicing/modules/slice_combiner.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  for slice_pred_name in slice_pred_op_names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 38s, sys: 13.3 s, total: 13min 52s\n",
      "Wall time: 30.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'WiC/SuperGLUE/valid/accuracy': 0.7554858934169278,\n",
       " 'WiC_slice:SF_verb_ind/SuperGLUE/valid/f1': 0.5493230174081237,\n",
       " 'WiC_slice:SF_verb_pred/SuperGLUE/valid/accuracy': 0.51440329218107,\n",
       " 'WiC_slice:base_ind/SuperGLUE/valid/f1': 1.0,\n",
       " 'WiC_slice:base_pred/SuperGLUE/valid/accuracy': 0.7570532915360502}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "slice_model.score(dataloaders[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some simple error analysis and an interface to specifying which _slice_ of the data we care about, we've improved our model **0.94 accuracy points** over a previous state-of-the-art model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
